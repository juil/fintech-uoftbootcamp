{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity: Comparing Imbalanced Classifiers\n",
    "\n",
    "In this activity, you’ll fit various balanced and imbalanced models to small business loan data. You’ll then compare the results by using the metrics that you’ve learned.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The U.S. Small Business Administration (SBA) is a government agency that exists to support the creation and growth of small companies. The SBA accomplishes this growth in several ways, one of which involves lending to these firms.\n",
    "\n",
    "The dataset for this activity contains information about actual small business loans that the SBA has issued. This dataset contains the following columns:\n",
    "\n",
    "- “Year”: The fiscal year of the loan application.\n",
    "\n",
    "- “Month”: The month of the fiscal year.\n",
    "\n",
    "- “Amount”: The issued loan amount.\n",
    "\n",
    "- “Term”: The term of the loan, in months.\n",
    "\n",
    "- “Zip”: The borrower’s zip code.\n",
    "\n",
    "- “CreateJob”: The number of jobs that were created by using the loan.\n",
    "\n",
    "- “NoEmp”: The number of business employees.\n",
    "\n",
    "- “RealEstate”: Whether the loan is backed by real estate.\n",
    "\n",
    "- “RevLineCr”: Whether the loan is a revolving line of credit.\n",
    "\n",
    "- “UrbanRural”: The location type of the borrower.\n",
    "\n",
    "- “Default”: Whether the borrower defaulted on the loan (1) or not (0).\n",
    "\n",
    "This dataset is imbalanced. Failing to repay a loan (that is, when the “Default” value equals 1) occurred rarely compared to the number of loans that borrowers successfully repaid.\n",
    "\n",
    "Using some of these variables as features, you need to try various models to find the one which can best predict which SBA loans are most likely to default.\n",
    "\n",
    "## Files\n",
    "\n",
    "Use the Jupyter notebook file in the `Unsolved` folder to write your code. The `Resources` folder contains the CSV file that you’ll import.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "1. Read in the CSV file from the `Resources` folder into a Pandas DataFrame.\n",
    "\n",
    "2. Create a Series named `y` that contains the data from the \"Default\" column of the original DataFrame. Note that this Series will contain the labels. Create a new DataFrame named `X` that contains the remaining columns from the original DataFrame. Note that this DataFrame will contain the features.\n",
    "\n",
    "3. Split the features and labels into training and testing sets, and `StandardScaler` your `X` data.\n",
    "\n",
    "4. Check the magnitude of imbalance in the dataset by viewing the number of distinct values (`value_counts`) for the labels. \n",
    "\n",
    "5. Fit two versions of a random forest model to the data: the first, a regular `RandomForest` classifier, and the second, a `BalancedRandomForest` classifier.\n",
    "\n",
    "6. Resample and fit the training data by one additional method for imbalanced data, such as `RandomOverSampler`, undersampling, or a synthetic technique.\n",
    "\n",
    "7. Print the confusion matrixes, accuracy scores, and classification reports for the three different models.\n",
    "\n",
    "8. Evaluate the effectiveness of `RandomForest`, `BalancedRandomForest`, and your one additional imbalanced classifier for predicting the minority class. \n",
    "    * Answer the following question: Does the model generated using one of the resampled classifiers more accurately flag all the loans that eventually defaulted?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.metrics import classification_report_imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read in the CSV file from the `Resources` folder into a Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Term</th>\n",
       "      <th>Zip</th>\n",
       "      <th>CreateJob</th>\n",
       "      <th>NoEmp</th>\n",
       "      <th>RealEstate</th>\n",
       "      <th>RevLineCr</th>\n",
       "      <th>UrbanRural</th>\n",
       "      <th>Default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001</td>\n",
       "      <td>11</td>\n",
       "      <td>32812</td>\n",
       "      <td>36</td>\n",
       "      <td>92801</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>30000</td>\n",
       "      <td>56</td>\n",
       "      <td>90505</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>30000</td>\n",
       "      <td>36</td>\n",
       "      <td>92103</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003</td>\n",
       "      <td>10</td>\n",
       "      <td>50000</td>\n",
       "      <td>36</td>\n",
       "      <td>92108</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006</td>\n",
       "      <td>7</td>\n",
       "      <td>343000</td>\n",
       "      <td>240</td>\n",
       "      <td>91345</td>\n",
       "      <td>3</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  Amount  Term    Zip  CreateJob  NoEmp  RealEstate  RevLineCr  \\\n",
       "0  2001     11   32812    36  92801          0      1           0          1   \n",
       "1  2001      4   30000    56  90505          0      1           0          1   \n",
       "2  2001      4   30000    36  92103          0     10           0          1   \n",
       "3  2003     10   50000    36  92108          0      6           0          1   \n",
       "4  2006      7  343000   240  91345          3     65           1          0   \n",
       "\n",
       "   UrbanRural  Default  \n",
       "0           0        0  \n",
       "1           0        0  \n",
       "2           0        0  \n",
       "3           0        0  \n",
       "4           2        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the sba_loans.csv file from the Resources folder into a Pandas DataFrame\n",
    "loans_df = pd.read_csv(\n",
    "    Path('../Resources/sba_loans.csv')\n",
    ")\n",
    "\n",
    "# Review the DataFrame\n",
    "loans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create a Series named `y` that contains the data from the \"Default\" column of the original DataFrame. Note that this Series will contain the labels. Create a new DataFrame named `X` that contains the remaining columns from the original DataFrame. Note that this DataFrame will contain the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X (features) and y (lables)\n",
    "\n",
    "# The y variable should focus on the Default column\n",
    "y = loans_df['Default']\n",
    "\n",
    "# The X variable should include all features except the Default column\n",
    "X = loans_df.drop(columns=['Default'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split the features and labels into training and testing sets, and `StandardScaler` your X data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_scaler = scaler.fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Check the magnitude of imbalance in the data set by viewing  the number of distinct values  (`value_counts`) for the labels. Scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1063\n",
       "1      96\n",
       "Name: Default, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the distinct values in the original labels data\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Fit two versions of a random forest model to the data: the first, a regular `RandomForest` classifier, and the second, a `BalancedRandomForest` classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions using the testing data\n",
    "rf_predictions = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BalancedRandomForestClassifier()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import BalancedRandomForestClassifier from imblearn\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# Instantiate a BalancedRandomForestClassifier instance\n",
    "brf = BalancedRandomForestClassifier()\n",
    "\n",
    "# Fit the model to the training data\n",
    "brf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for testing features\n",
    "brf_predictions = brf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Resample and fit the training data by one additional method for imbalanced data, such as `RandomOverSampler`, undersampling, or a synthetic technique. Re-esimate by `RandomForest`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SMOTE from imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Instantiate the SMOTE model instance\n",
    "smote_sampler = SMOTE(random_state=1, sampling_strategy='auto')\n",
    "\n",
    "# Fit the SMOTE model to the training data\n",
    "X_resampled, y_resampled = smote_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Fit the RandomForestClassifier on the resampled data\n",
    "model_resampled_rf = RandomForestClassifier()\n",
    "model_resampled_rf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Generate predictions based on the resampled data model\n",
    "rf_resampled_predictions = model_resampled_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Print the confusion matrixes, accuracy scores, and classification reports for the three different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[338,  10],\n",
       "       [ 13,  26]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the confusion matrix for RandomForest on the original data\n",
    "confusion_matrix(y_test, rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[315,  33],\n",
       "       [  5,  34]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the confusion matrix for balanced random forest data\n",
    "confusion_matrix(y_test, brf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[329,  19],\n",
       "       [  7,  32]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the confusion matrix for RandomForest on the resampled data\n",
    "confusion_matrix(y_test, rf_resampled_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8189655172413792\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy score for the original data\n",
    "baso = balanced_accuracy_score(y_test, rf_predictions)\n",
    "print(baso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8884836427939876\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy score for the resampled data\n",
    "basr = balanced_accuracy_score(y_test, brf_predictions)\n",
    "print(basr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8829575596816976\n"
     ]
    }
   ],
   "source": [
    "# Print the accuracy score for the resampled data\n",
    "basrs = balanced_accuracy_score(y_test, rf_resampled_predictions)\n",
    "print(basrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.96      0.97      0.67      0.97      0.80      0.67       348\n",
      "          1       0.72      0.67      0.97      0.69      0.80      0.63        39\n",
      "\n",
      "avg / total       0.94      0.94      0.70      0.94      0.80      0.66       387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report for the original data\n",
    "print(classification_report_imbalanced(y_test, rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.98      0.91      0.87      0.94      0.89      0.79       348\n",
      "          1       0.51      0.87      0.91      0.64      0.89      0.79        39\n",
      "\n",
      "avg / total       0.94      0.90      0.88      0.91      0.89      0.79       387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report for the resampled data\n",
    "print(classification_report_imbalanced(y_test, brf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.98      0.95      0.82      0.96      0.88      0.79       348\n",
      "          1       0.63      0.82      0.95      0.71      0.88      0.77        39\n",
      "\n",
      "avg / total       0.94      0.93      0.83      0.94      0.88      0.78       387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report for the resampled data\n",
    "print(classification_report_imbalanced(y_test, rf_resampled_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate the effectiveness of `RandomForest`, `BalancedRandomForest`, and your one additional imbalanced classifier for predicting the minority class. \n",
    "\n",
    "### Answer the following question: Does the model generated using one of the imbalanced methods more accurately flag all the loans that eventually defaulted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Does the model generated using one of the imbalanced methods more accurately flag all the loans that eventually defaulted?\n",
    "    \n",
    "**Answer:** Overall, both resampled models in this example perform better at identifying more of the eventual loan defaults. We can see this by looking at the increase recall for the “default” or “`1`” category in the two imbalanced models, when compared to the  original random forest model.\n",
    "\n",
    "A higher recall for this category means that of all the loans that actually were in default, how many did this model correctly catch? A higher recall for a model means it’s going to do a better job at making sure any potential defaults are not missed.\n",
    "\n",
    "However, the higher recall for these two imbalanced models comes at a cost: a greater tendency to flag a loan as a potential default, even when it does not. This is evidenced by a lower precision for these two models. If precision looks at, of all those loans the model predicted as default, how many of them actually were defaults, then a lower precision value means that the model is making a lot of false positives; predicting a default when there isn’t actually one. \n",
    "\n",
    "This illustrates the main tradeoff when using imbalanced versions of machine learning models. If you really care about identifying those faulty loans (or whatever you’re trying to predict), and the cost of failing to identify a faulty loan is very high, then maybe an imbalanced model with lower precision is worth it. After all, we can always find another business to lend to, but if that business defaults, it’s very costly to us as a lender. \n",
    "\n",
    "If on the other hand, you have a situation in which the costs of misclassification are the same either way—if failing to correctly identify a `1` has the same practical cost as failing to correctly identify a `0`—then we may be better off with the overall higher accuracy of a standard machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
